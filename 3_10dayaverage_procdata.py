# -*- coding: utf-8 -*-
"""3_10dayAverage_procdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_rcgeknAy4ZdBK55Su5I0scH12a9kQM
"""





from google.colab import drive
drive.mount('/content/drive')



"""**10 day averaging**"""





# DIAGNOSTIC: Check CSV file
import pandas as pd

CSV_FILE = '/content/drive/MyDrive/Google Earth/Adelite_bands/Sentinel1_adelite/processed_res3_data.csv'

print("=" * 80)
print("CSV FILE DIAGNOSTIC")
print("=" * 80)

try:
    df_test = pd.read_csv(CSV_FILE)

    print(f"\n✓ File loaded successfully")
    print(f"  Rows: {len(df_test)}")
    print(f"  Columns: {list(df_test.columns)}")

    print(f"\n Data Statistics:")
    print(f"  mavg range: {df_test['mavg'].min():.6f} to {df_test['mavg'].max():.6f}")
    print(f"  base range: {df_test['base'].min():.2f} to {df_test['base'].max():.2f}")
    print(f"  mavg mean: {df_test['mavg'].mean():.6f}")
    print(f"  base mean: {df_test['base'].mean():.2f}")

    print(f"\n Issue Check:")
    print(f"  Cells with mavg = 0: {(df_test['mavg'] == 0).sum()}")
    print(f"  Cells with mavg < 0.01: {(df_test['mavg'] < 0.01).sum()}")
    print(f"  Countries: {df_test['country'].unique()}")

    print(f"\n First 5 rows:")
    print(df_test.head())

    print(f"\n Top 5 cells by mavg (most variable):")
    print(df_test.nlargest(5, 'mavg')[['h3_3', 'mavg', 'base', 'country']])

    # THE CRITICAL TEST
    print(f"\n" + "=" * 80)
    print("SIMULATING DAY 11 SPIKE:")
    print("=" * 80)

    df_day11 = df_test.copy()
    cells_to_spike = df_day11.head(20)['h3_3'].values

    print(f"\nBEFORE spike:")
    sample_cell = cells_to_spike[0]
    before = df_day11[df_day11['h3_3'] == sample_cell][['mavg', 'base']].iloc[0]
    print(f"  Cell {sample_cell[:15]}...")
    print(f"    mavg: {before['mavg']:.6f}")
    print(f"    base: {before['base']:.2f}")

    # Apply spike
    for cell in cells_to_spike:
        mask = df_day11['h3_3'] == cell
        df_day11.loc[mask, 'mavg'] = df_day11.loc[mask, 'mavg'] + 20
        df_day11.loc[mask, 'base'] = df_day11.loc[mask, 'base'] + 40

    print(f"\nAFTER spike:")
    after = df_day11[df_day11['h3_3'] == sample_cell][['mavg', 'base']].iloc[0]
    print(f"  Cell {sample_cell[:15]}...")
    print(f"    mavg: {after['mavg']:.6f} (+{after['mavg'] - before['mavg']:.2f})")
    print(f"    base: {after['base']:.2f} (+{after['base'] - before['base']:.2f})")

    print(f"\n✓ Spike simulation working correctly!")

except FileNotFoundError:
    print(f"\n✗ ERROR: File not found!")
    print(f"  Path: {CSV_FILE}")
    print(f"\nCheck:")
    print(f"  1. Is Google Drive mounted?")
    print(f"  2. Is the path correct?")

except Exception as e:
    print(f"\n✗ ERROR: {e}")

print("=" * 80)







import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.mixture import GaussianMixture

# ===== CONFIGURATION =====
NUM_DAYS = 30  # Extended to show transitions better
INTERPOLATION_DAYS = 10  # Number of days to interpolate over (d)
CLUSTER_JUMP_THRESHOLD = 1  # Very low threshold since data has small base values
CSV_FILE = '/content/drive/MyDrive/Google Earth/Adelite_bands/Sentinel1_adelite/processed_res3_data.csv'

# Severity levels
THRESHOLD_LEVELS = {
    'threshold_high': 0.70,
    'threshold_medium': 0.50,
    'threshold_low': 0.10
}

print("=" * 80)
print("ADVANCED THRESHOLD SYSTEM WITH CLUSTERING & INTERPOLATION")
print("=" * 80)
print(f"\nConfiguration:")
print(f"  - Days to simulate: {NUM_DAYS}")
print(f"  - Interpolation period: {INTERPOLATION_DAYS} days")
print(f"  - Cluster jump threshold: {CLUSTER_JUMP_THRESHOLD}")

# ===== HELPER FUNCTIONS =====

def simulate_h3_children(res3_cell, num_children=3):
    """Simulate creating res8 children from a res3 parent."""
    children = []
    for i in range(num_children):
        children.append(f"{res3_cell}_{i:03d}")
    return children

def simulate_h3_to_parent(res8_cell):
    """Simulate getting res3 parent from res8 cell."""
    return res8_cell.split('_')[0]

def train_clusters_full(df):
    """
    Full clustering implementation based on train_clusters() from thresholds_build.py
    Returns scalers, GMM model, and cluster assignments
    """
    df = df.copy()

    # Apply mavg logic - MODIFIED to preserve spikes
    # Use max() to preserve large mavg values from spikes
    df['mavg_processed'] = df.apply(
        lambda x: max(x['mavg'], x['base'] * 0.05),
        axis=1
    )

    # Standardize
    scaler_1 = StandardScaler()
    scaler_2 = StandardScaler()

    df['mavg_scaled'] = scaler_1.fit_transform(df['mavg_processed'].values.reshape(-1, 1))
    df['base_scaled'] = scaler_2.fit_transform(df['base'].values.reshape(-1, 1))

    # Polynomial features
    X = df[['base_scaled', 'mavg_scaled']].values
    poly = PolynomialFeatures(degree=3, include_bias=False)
    X_poly = poly.fit_transform(X)

    # GMM clustering
    n_components = min(40, max(2, len(df) // 2))
    gmm = GaussianMixture(n_components=n_components, random_state=0).fit(X_poly)
    df['cluster'] = gmm.predict(X_poly)

    return scaler_1, scaler_2, gmm, poly, df

def calculate_cluster_thresholds(scaler_1, scaler_2, gmm, df):
    """
    Calculate thresholds based on GMM cluster characteristics
    Replicates calculate_thresholds() logic from thresholds_build.py
    """
    # Extract cluster characteristics
    variance_mavg = scaler_1.inverse_transform(gmm.covariances_[:, 1, 1].reshape(-1, 1)).flatten()
    covariance_mavg_base = gmm.covariances_[:, 0, 0].reshape(-1, 1).flatten()
    cluster_means = scaler_2.inverse_transform(gmm.means_[:, 0].reshape(-1, 1)).flatten()

    # Calculate factors for different severity levels
    factor_high = variance_mavg + np.multiply(covariance_mavg_base, cluster_means) * 5.4
    factor_medium = variance_mavg + np.multiply(covariance_mavg_base, cluster_means) * 3.8
    factor_low = variance_mavg + np.multiply(covariance_mavg_base, cluster_means) * 2.2

    # Calculate thresholds
    base_unscaled = scaler_2.inverse_transform(df['base_scaled'].values.reshape(-1, 1)).flatten()

    df['threshold_high'] = base_unscaled + factor_high[df['cluster']]
    df['threshold_medium'] = base_unscaled + factor_medium[df['cluster']]
    df['threshold_low'] = base_unscaled + factor_low[df['cluster']]

    return df

def linear_interpolate(old_value, new_value, current_step, total_steps):
    """
    Linear interpolation from old_value to new_value
    current_step: 0 to total_steps (0 = old_value, total_steps = new_value)
    """
    if total_steps == 0:
        return new_value
    progress = min(current_step / total_steps, 1.0)
    return old_value + (new_value - old_value) * progress

# ===== STEP 1: Load res3 data =====
print("\n" + "=" * 80)
print("STEP 1: Loading res3 data")
print("=" * 80)

df_res3 = pd.read_csv(CSV_FILE)
print(f"✓ Loaded {len(df_res3)} res3 cells")
print(f"  Columns: {list(df_res3.columns)}")
print(f"  mavg range: {df_res3['mavg'].min():.6f} to {df_res3['mavg'].max():.6f}")
print(f"  base range: {df_res3['base'].min():.2f} to {df_res3['base'].max():.2f}")

# ===== STEP 2: Simulate daily clustering and threshold calculation =====
print("\n" + "=" * 80)
print("STEP 2: Daily clustering and threshold calculation")
print("=" * 80)

all_daily_data = []
cluster_history = {}  # Track cluster assignments over time

for day in range(1, NUM_DAYS + 1):
    if day % 5 == 0:  # Print every 5 days to reduce clutter
        print(f"Processing day {day}/{NUM_DAYS}...")

    # Start with original data
    df_day = df_res3.copy()

    # Trigger major spike on day 11
    if day == 11:
        print(f"\n  🔥 DAY 11: SIMULATING MAJOR ACTIVITY SPIKE 🔥")
        # Select first 20 cells and dramatically increase their activity
        cells_with_influx = df_day.head(20)['h3_3'].values

        print(f"  → Spiking {len(cells_with_influx)} cells")

        # Show before/after for first cell
        sample_cell = cells_with_influx[0]
        before_mavg = df_day.loc[df_day['h3_3'] == sample_cell, 'mavg'].values[0]
        before_base = df_day.loc[df_day['h3_3'] == sample_cell, 'base'].values[0]

        # Apply massive spike
        for cell in cells_with_influx:
            mask = df_day['h3_3'] == cell
            df_day.loc[mask, 'mavg'] = df_day.loc[mask, 'mavg'] + 20.0  # Large absolute increase
            df_day.loc[mask, 'base'] = df_day.loc[mask, 'base'] + 40.0  # Large base increase

        after_mavg = df_day.loc[df_day['h3_3'] == sample_cell, 'mavg'].values[0]
        after_base = df_day.loc[df_day['h3_3'] == sample_cell, 'base'].values[0]

        print(f"  → Sample cell {sample_cell[:15]}...")
        print(f"     Before: mavg={before_mavg:.4f}, base={before_base:.2f}")
        print(f"     After:  mavg={after_mavg:.4f}, base={after_base:.2f}")
        print(f"     Change: mavg +{after_mavg-before_mavg:.2f}, base +{after_base-before_base:.2f}")

    # Train clusters for this day
    scaler_1, scaler_2, gmm, poly, df_clustered = train_clusters_full(df_day)

    # Calculate thresholds
    df_with_thresholds = calculate_cluster_thresholds(scaler_1, scaler_2, gmm, df_clustered)

    # Store daily data
    for _, row in df_with_thresholds.iterrows():
        h3_3 = row['h3_3']
        country = row['country']

        # Track cluster history
        key = (h3_3, country)
        if key not in cluster_history:
            cluster_history[key] = []
        cluster_history[key].append({
            'day': day,
            'cluster': row['cluster'],
            'threshold_high': row['threshold_high'],
            'threshold_medium': row['threshold_medium'],
            'threshold_low': row['threshold_low']
        })

        all_daily_data.append({
            'day': day,
            'h3_3': h3_3,
            'country': country,
            'cluster': row['cluster'],
            'base': row['base'],
            'mavg': row['mavg'],
            'threshold_high_raw': row['threshold_high'],
            'threshold_medium_raw': row['threshold_medium'],
            'threshold_low_raw': row['threshold_low']
        })

df_all_daily = pd.DataFrame(all_daily_data)
print(f"\n✓ Generated {len(df_all_daily)} daily records")

# ===== STEP 3: Detect cluster transitions =====
print("\n" + "=" * 80)
print("STEP 3: Detecting cluster transitions")
print("=" * 80)

transitions = []
cluster_changes_count = 0

for key, history in cluster_history.items():
    h3_3, country = key

    for i in range(1, len(history)):
        prev_day = history[i-1]
        curr_day = history[i]

        # Check if cluster changed
        if prev_day['cluster'] != curr_day['cluster']:
            cluster_changes_count += 1

            # Check if threshold jump is significant
            threshold_jump = abs(curr_day['threshold_high'] - prev_day['threshold_high'])

            # Print first few transitions for debugging
            if len(transitions) < 5:
                print(f"  Cluster change: {h3_3[:15]}... Day {curr_day['day']}: "
                      f"C{prev_day['cluster']}→C{curr_day['cluster']}, "
                      f"Threshold jump: {threshold_jump:.2f}")

            if threshold_jump > CLUSTER_JUMP_THRESHOLD:
                transitions.append({
                    'h3_3': h3_3,
                    'country': country,
                    'day': curr_day['day'],
                    'old_cluster': prev_day['cluster'],
                    'new_cluster': curr_day['cluster'],
                    'old_threshold_high': prev_day['threshold_high'],
                    'new_threshold_high': curr_day['threshold_high'],
                    'threshold_jump': threshold_jump
                })

df_transitions = pd.DataFrame(transitions)
print(f"\n✓ Total cluster changes: {cluster_changes_count}")
print(f"✓ Significant transitions (jump > {CLUSTER_JUMP_THRESHOLD}): {len(df_transitions)}")

if len(df_transitions) > 0:
    print("\nTop 5 transitions by threshold jump:")
    print(df_transitions.nlargest(5, 'threshold_jump')[['h3_3', 'day', 'old_cluster', 'new_cluster', 'threshold_jump']])

# ===== STEP 4: Apply interpolation for smooth transitions =====
print("\n" + "=" * 80)
print("STEP 4: Applying linear interpolation for smooth transitions")
print("=" * 80)

# Create a copy for interpolated values
df_interpolated = df_all_daily.copy()
df_interpolated['threshold_high'] = df_interpolated['threshold_high_raw']
df_interpolated['threshold_medium'] = df_interpolated['threshold_medium_raw']
df_interpolated['threshold_low'] = df_interpolated['threshold_low_raw']
df_interpolated['is_interpolated'] = False

# Apply interpolation for each transition
for idx, transition in df_transitions.iterrows():
    h3_3 = transition['h3_3']
    country = transition['country']
    transition_day = transition['day']

    # Get old and new thresholds for all levels
    old_data = df_all_daily[
        (df_all_daily['h3_3'] == h3_3) &
        (df_all_daily['country'] == country) &
        (df_all_daily['day'] == transition_day - 1)
    ]

    new_data = df_all_daily[
        (df_all_daily['h3_3'] == h3_3) &
        (df_all_daily['country'] == country) &
        (df_all_daily['day'] == transition_day)
    ]

    if len(old_data) == 0 or len(new_data) == 0:
        continue

    old_data = old_data.iloc[0]
    new_data = new_data.iloc[0]

    if idx == 0:  # Print details for first transition
        print(f"\nInterpolating transition for {h3_3[:15]}... on day {transition_day}")
        print(f"  Old threshold: {old_data['threshold_high_raw']:.2f}")
        print(f"  New threshold: {new_data['threshold_high_raw']:.2f}")
        print(f"  Will interpolate over {INTERPOLATION_DAYS} days")

    # Interpolate over INTERPOLATION_DAYS
    for step in range(INTERPOLATION_DAYS + 1):
        target_day = transition_day + step
        if target_day > NUM_DAYS:
            break

        # Calculate interpolated values
        interp_high = linear_interpolate(
            old_data['threshold_high_raw'],
            new_data['threshold_high_raw'],
            step,
            INTERPOLATION_DAYS
        )
        interp_medium = linear_interpolate(
            old_data['threshold_medium_raw'],
            new_data['threshold_medium_raw'],
            step,
            INTERPOLATION_DAYS
        )
        interp_low = linear_interpolate(
            old_data['threshold_low_raw'],
            new_data['threshold_low_raw'],
            step,
            INTERPOLATION_DAYS
        )

        # Update the dataframe
        mask = (
            (df_interpolated['h3_3'] == h3_3) &
            (df_interpolated['country'] == country) &
            (df_interpolated['day'] == target_day)
        )

        df_interpolated.loc[mask, 'threshold_high'] = interp_high
        df_interpolated.loc[mask, 'threshold_medium'] = interp_medium
        df_interpolated.loc[mask, 'threshold_low'] = interp_low
        df_interpolated.loc[mask, 'is_interpolated'] = True

print(f"\n✓ Applied interpolation to {df_interpolated['is_interpolated'].sum()} records")

# ===== STEP 5: Calculate final averaged thresholds =====
print("\n" + "=" * 80)
print("STEP 5: Calculating final 10-day averaged thresholds")
print("=" * 80)

# Use last 10 days for averaging
last_10_days = df_interpolated[df_interpolated['day'] > NUM_DAYS - 10]

df_avg_thresholds = last_10_days.groupby(['h3_3', 'country']).agg({
    'threshold_high': 'mean',
    'threshold_medium': 'mean',
    'threshold_low': 'mean'
}).reset_index()

print(f"✓ Calculated averaged thresholds for {len(df_avg_thresholds)} res3 cells")

# ===== STEP 6: Create assets and assign thresholds =====
print("\n" + "=" * 80)
print("STEP 6: Creating assets and assigning thresholds")
print("=" * 80)

sample_assets = []
for idx, row in df_res3.head(10).iterrows():
    res8_children = simulate_h3_children(row['h3_3'], num_children=2)
    for asset_idx, res8_cell in enumerate(res8_children):
        sample_assets.append({
            'asset_id': f"asset_{idx}_{asset_idx}",
            'h3_res8': res8_cell,
            'country': row['country']
        })

df_assets = pd.DataFrame(sample_assets)
df_assets['h3_res3_parent'] = df_assets['h3_res8'].apply(simulate_h3_to_parent)

# Merge with averaged thresholds
df_final = df_assets.merge(
    df_avg_thresholds,
    left_on=['h3_res3_parent', 'country'],
    right_on=['h3_3', 'country'],
    how='left'
)

print(f"✓ Created {len(df_final)} assets with smooth, averaged thresholds")
print("\nSample final assets:")
print(df_final[['asset_id', 'h3_res3_parent', 'threshold_high', 'threshold_medium', 'threshold_low']].head())

# ===== STEP 7: Visualization of interpolation effect =====
print("\n" + "=" * 80)
print("STEP 7: Demonstrating interpolation effect")
print("=" * 80)

if len(df_transitions) > 0:
    # Pick the first transition to visualize
    sample_transition = df_transitions.iloc[0]
    h3_3 = sample_transition['h3_3']
    country = sample_transition['country']

    # Get data for this cell
    cell_data = df_all_daily[
        (df_all_daily['h3_3'] == h3_3) &
        (df_all_daily['country'] == country)
    ][['day', 'cluster', 'threshold_high_raw']].copy()

    cell_interp = df_interpolated[
        (df_interpolated['h3_3'] == h3_3) &
        (df_interpolated['country'] == country)
    ][['day', 'threshold_high', 'is_interpolated']].copy()

    # Merge
    comparison = cell_data.merge(cell_interp, on='day')

    print(f"\n📊 Cell: {h3_3}")
    print(f"   Transition occurred on day {sample_transition['day']}")
    print(f"   Cluster changed: {sample_transition['old_cluster']} → {sample_transition['new_cluster']}")
    print(f"\n   Comparison of raw vs interpolated thresholds:")
    print(comparison[['day', 'cluster', 'threshold_high_raw', 'threshold_high', 'is_interpolated']].to_string(index=False))
else:
    print("\n⚠️  No transitions detected to visualize")

# ===== STEP 8: Summary =====
print("\n" + "=" * 80)
print("SUMMARY")
print("=" * 80)

print(f"\n1. Processed {NUM_DAYS} days of data with daily clustering")
print(f"2. Detected {cluster_changes_count} total cluster changes")
print(f"3. Found {len(df_transitions)} significant transitions (threshold jump > {CLUSTER_JUMP_THRESHOLD})")
print(f"4. Applied {INTERPOLATION_DAYS}-day linear interpolation to smooth transitions")
print(f"5. Interpolated {df_interpolated['is_interpolated'].sum()} records")
print(f"6. Calculated 10-day averaged thresholds for {len(df_avg_thresholds)} res3 cells")
print(f"7. Assigned smooth thresholds to {len(df_final)} assets")

print("\n" + "=" * 80)
print("✅ COMPLETE: Advanced system with clustering, transitions, and interpolation")
print("=" * 80)







